Configuration files with the hyper-parameters for all models

BERT : 
    -> Learning rate : 1e-5
    -> Max_epoch = 3 epoch
    -> Batch size = 16
    -> Simple square transformation for the class weights
    
BERT with KD features :
    -> Learning rate : 1e-5
    -> Max_epoch = 3 epoch
    -> Batch size = 16
    -> Simple square transformation for the class weights
    -> PCA on KD Features (d=100)

GBoost KD Features :

    -> Double square root weight transformation
    -> No PCA
    -> Rule based label only model:
        parameters = {"n_estimators" : 2000, 
                "num_leaves" : 27,
                "learning_rate" : 0.087,
                "reg_alpha" : 0.462,
                "reg_lambda" : 0.660,
                "objective": "multi:softmax",
                "n_jobs" : 6,
                "verbose" : -1}
        
    -> Rule-based + n-gram model 
        
        parameters = {"n_estimators" : 2000, 
                "num_leaves" : 22,
                "learning_rate" : 0.096,
                "reg_alpha" : 0.630,
                "reg_lambda" : 0.451,
                "objective": "multi:softmax",
                "n_jobs" : 6,
                "verbose" : -1}
            
    -> Rule-based + n-gram model + pos-tag
        parameters = {"n_estimators" : 2000, 
                "num_leaves" : 87,
                "learning_rate" : 0.081,
                "reg_alpha" : 0.670,
                "reg_lambda" : 0.357,
                "objective": "multi:softmax",
                "n_jobs" : 6,
                "verbose" : -1}
        
    -> Rule-based + n-gram model + pos-tag + liwc

        parameters = {"n_estimators" : 2000, 
                "num_leaves" : 35,
                "learning_rate" : 0.075,
                "reg_alpha" : 0.572,
                "reg_lambda" : 0.487,
                "objective": "multi:softmax",
                "n_jobs" : 6,
                "verbose" : -1}

    -> Rule-based + ngram + pos-tag + liwc + tutoring moves
        parameters = {"n_estimators" : 2000, 
                "num_leaves" : 29,
                "learning_rate" : 0.0029,
                "reg_alpha" : 0.343,
                "reg_lambda" : 0.639,
                "objective": "multi:softmax",
                "n_jobs" : 6,
                "verbose" : -1
            } 
        
    -> Rule-based + ngram + pos-tag + liwc + tutoring moves + nvb = all features
        parameters = {"n_estimators" : 2000, 
            "num_leaves" : 75,
            "learning_rate" : 0.0024,
            "reg_alpha" : 0.424,
            "reg_lambda" : 0.500,
            "objective": "multi:softmax",
            "n_jobs" : 6,
            "verbose" : -1
        } 

GBoost SentBERT : 

    -> Double square root transformation
    -> No PCA
    -> model_embeddings = SentenceTransformer('all-mpnet-base-v2') (d=768)
    -> parameters = {"n_estimators" : 3000, 
                "num_leaves" : 6,
                "learning_rate" : 0.038,
                "reg_alpha" : 0.1,
                "reg_lambda" : 0.232,
                "objective": "multi:softmax",
                "n_jobs" : 6,
                "verbose" : -1
            } 


GBoost KD Features + SentBERT :

    -> Double square root transformation
    -> No PCA for embeddings, PCA for KD Features 
    -> model_embeddings = SentenceTransformer('all-mpnet-base-v2') (d=768)
    -> parameters = {"n_estimators" : 3000, 
                "num_leaves" : 106,
                "learning_rate" : 0.041,
                "reg_alpha" : 0.143,
                "reg_lambda" : 0.296,
                "objective": "multi:softmax",
                "n_jobs" : 6,
                "verbose" : -1
            } 

MLP KD Features :

    -> 'dropout': 0.18, 'layer_1_hidden_size': 42, 'layer_2_hidden_size': 24, 'lr': 0.0036
    -> Simple square transformation for the class weights
    -> PCA on KD Features (d=100)
    -> Batch size = 32
    -> Max_epoch = 100

MLP SentBERT :

    -> 'dropout': 0.20, 'layer_1_hidden_size': 215, 'layer_2_hidden_size': 87, 'lr': 0.0024
    -> Simple square transformation for the class weights
    -> No PCA
    -> Max_epoch = 100
    -> Batch size = 32

MLP KD Features + SentBERT :

    -> 'dropout': 0.18, 'layer_1_hidden_size': 477, 'layer_2_hidden_size': 106, 'lr': 0.0012
    -> Simple square transformation for the class weights
    -> PCA on KD Features (d=100), no PCA on Pre-trained embeddings
    -> Max_epoch = 100
    -> Batch size = 32

Attention CNN :

    -> 'out_channels_conv': 14, 'dropout': 0.2, 'channel_width': 3, 'lr': 0.001
    -> Batch size = 32
    -> Max_epoch = 10 

LSTM KD Features :

    -> 'size_history': 6, 'hidden_size': 73, 'lr': 0.0080, 1 layer
    -> Batch size = 32
    -> Max_epoch = 100
    -> PCA on KD Features (d=100)
    -> Simple square transformation for the class weights

LSTM SentBERT :

    -> 'size_history': 4, 'dropout': 0.17, 'hidden_size': 24, 'lr': 0.0047, 2 layers
    -> Batch size = 32
    -> Max_epoch = 100
    -> No PCA
    -> Simple square transformation for the class weights

LSTM KD Features + SentBERT :

    -> 'size_history': 5, 'hidden_size': 78, 'lr': 0.0058
    -> Batch size = 32
    -> Max_epoch = 100
    -> No PCA
    -> Simple square transformation for the class weights